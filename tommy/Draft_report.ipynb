{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 158.755 Data Science - Making Sense of Data Project 4\n",
    "# Study In Recommender Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Abstract\n",
    "\n",
    "A recommender system, or a recommender engine intends to seek prediction of 'rating' or 'preference' for a user that would rank an item or event, mainly used in commercial web applications. \n",
    "\n",
    "They have been utilized in various areas over the last ten years, commonly used as playlist generators for video and music services such as Youtube, Netflix, and BiliBili, product recommenders like Amazon, Trademe, and Ebay, and content or news based recommenders for social media platforms as Facebook, Twitter and Instergram. Also, there are other popular recommender systems being utilized for specific topics as hotels booking, dating matching, and online competition game team up.\n",
    "\n",
    "Flow and monetization are the core concept associated with commercial web applications for internet. Simply speaking, flow is the measurement that the number of visitings for a web application, while monetization evaluates its overall income for keeping the business up and running continuously and healthily. (i.e. advertisement income of Youtube, membership subscriptions for Netflix, and fees/charges for each successfully sale for Amazon ). \n",
    "\n",
    "Therefore, recommendation engine is one of the key part for monetization that allows finding consumers' real demand through directing them to their most interest items or services. In addition to this , owners of commercial web applications, would be able accurately deploy advertisements and services to their customers based on a successful recommendation engine set up, for example ,playlist generators (Youtube) could generate just right advertisements to their viewers , and product recommenders like Amazon could pushed customers related products when they are viewing a certain item. Thus, it would optimise the resource usage and amplify income for a commercial organisation who has developed such kind of commercial web applications. \n",
    "\n",
    "In this project , we would implement a ***'breath first'*** strategy so as to explore as much lifecycle and implementations for a recommendation engine , such as the input and output data structure, machine learning algorithms, evaluation standard for comparison of different algorithms, and its position and role in architecture of a commercial web applications , instead of undertaking ***in-depth*** research on specific algorithms as per their theory and implementations. Thus, we will simply use an existing library to compute our findings and recommendations for this project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Indroduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Data Collection\n",
    "\n",
    "The datasets for a commercial web application have almost never been exposed for public usage or research, as they are private and strategy assets for business. Normally they are generated through \n",
    "\n",
    "Fortunately, there are existing web source regarding recommendation engine study for us to play with , which is Amazon product data prepared by Associate Professor Julian McAuley, UCSD at http://jmcauley.ucsd.edu/data/amazon/.\n",
    "\n",
    "Normally, user activates for a commercial web app could be concluded as viewing a certain web page , purchasing products, comments and rank items , content or news etc. It is quite difficult to summarise a standard data structure for a recommendation engine due to the variety demands and requirements for various business activities.\n",
    "\n",
    "However, a classic data structure format has been widely used after long term experimentation and operation with major internet giant companies such as Amazon and Facebook:\n",
    "\n",
    "- ***user id*** : unique user id\n",
    "- ***item id*** : unique item id\n",
    "- ***behaviour type*** : type of behaviour , i.e. purchase or view an item\n",
    "- ***context*** : behaviour context, including location and time etc.\n",
    "- ***behaviour weight*** : weight could be the viewing length for a video or rank for an item \n",
    "- ***behaviour content*** : if a user comments something, the content could be saved as a text file. If user click an item, the content could be a binary input.\n",
    "\n",
    "\n",
    "### 3.1 Web API\n",
    "\n",
    "Amazon Instant Video review data , http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Amazon_Instant_Video_5.json.gz , has been selected as the dataset for this project. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 3.2 Web Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Algorithm Research\n",
    "\n",
    "As there are many approaches to implement recommender engine, such as Collaborative filtering, Content-based filtering, Multi-criteria recommender systems etc. Their brief concept and implementations could be refer to \n",
    "https://en.wikipedia.org/wiki/Recommender_system.\n",
    "\n",
    "In this project , we would use an existing library named as ***SurPRISE***, stands for ***Simple Python RecommendatIon System Engine.***,http://surpriselib.com/.\n",
    "\n",
    "As it has provided various ready-to-use prediction algorithms with good documentation and use cases, such as baseline algorithms, neighborhood methods, matrix factorization-based ( SVD, PMF, SVD++, NMF), and many others. Also, various similarity measures (cosine, MSD, pearson…) are built-in.\n",
    "\n",
    "In this section ,we will mainly focus on ***matrix factorization-based modules and similarity modules***.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Input Data Structure\n",
    "\n",
    "The input data structure is very simple , just a big user - item with rating matrix , that is represented as a specific user ranks an item with certain rating as per the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(497577, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(497577, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#full dataset for research\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('full_dataset.csv')\n",
    "# df = pd.read_csv('testForInput.csv')\n",
    "# df = pd.read_csv('test.csv')\n",
    "print(df.shape)\n",
    "# df = df[:1000]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1HP7NVNPFMA4N</td>\n",
       "      <td>0700026657</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1JGAP0185YJI6</td>\n",
       "      <td>0700026657</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1YJWEXHQBWK2B</td>\n",
       "      <td>0700026657</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2204E1TH211HT</td>\n",
       "      <td>0700026657</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2RF5B5H74JLPE</td>\n",
       "      <td>0700026657</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           userID      itemID  rating\n",
       "0  A1HP7NVNPFMA4N  0700026657       5\n",
       "1  A1JGAP0185YJI6  0700026657       4\n",
       "2  A1YJWEXHQBWK2B  0700026657       3\n",
       "3  A2204E1TH211HT  0700026657       2\n",
       "4  A2RF5B5H74JLPE  0700026657       5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 497577 entries, 0 to 497576\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   userID  497577 non-null  object\n",
      " 1   itemID  497577 non-null  object\n",
      " 2   rating  497577 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 11.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Performance measures\n",
    "\n",
    "The commonly used metrics are the ***Mean Absolute Error(MAE)*** and ***Root Mean Squared Error(RMSE)***,  ***precision*** and ***recall*** will also be used to evaluate the quality of a model for comparison. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  6.0 Matrix Factorization-Based Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from surprise import Dataset\n",
    "import pandas as pd\n",
    "from surprise import SVD\n",
    "from surprise import NormalPredictor\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "import numpy as np\n",
    "from surprise import dump\n",
    "import os\n",
    "from surprise.model_selection import KFold\n",
    "import io  # needed because of weird encoding of u.item file\n",
    "\n",
    "from surprise import KNNBaseline\n",
    "from surprise import get_dataset_dir\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    '''Return precision and recall at k metrics for each user.'''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "\n",
    "    return precisions, recalls\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('full_dataset.csv')\n",
    "# A reader is still needed but only the rating_scale param is requiered.\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(df[['userID', 'itemID', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 SVD\n",
    "\n",
    "The famous SVD algorithm, which was popularized by Simon Funk during the Netflix Prize in 2006. It's documentation and reference could be referred to links as below:\n",
    "\n",
    "https://sifter.org/~simon/journal/20061211.html\n",
    "\n",
    "https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD\n",
    "\n",
    "SVD is stand for singular value decomposition mathmatically, is a factorization of a real or complex matrix that generalizes the eigendecomposition of a square normal matrix to any m × n matrix via an extension of the polar decomposition. \n",
    "\n",
    "Actually , it is dry and headache to focus on the mathmatical details for this algorithm, ***SurPRISE*** lib has encapsulated ready to use functions to implement this approach.\n",
    "\n",
    "The code below is a standard machine learning process , which has iterated through all combinations of parameters in ***param_grid*** variable with K folds method (3 folds), so as to find the best prediction model based on RMSE and MAE score.\n",
    "\n",
    "***{'n_epochs': 15, 'lr_all': 0.01, 'reg_all': 0.2}*** parameters have been found as the best score model for this approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_epochs': [5, 10,15], 'lr_all': [0.002, 0.005,0.01],\n",
    "              'reg_all': [0.2, 0.4, 0.6]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best RMSE score\n",
      "1.0272657422404547\n",
      "0.7620971595317978\n",
      "combination of parameters that gave the best RMSE score\n",
      "{'n_epochs': 15, 'lr_all': 0.01, 'reg_all': 0.2}\n",
      "{'n_epochs': 15, 'lr_all': 0.01, 'reg_all': 0.2}\n"
     ]
    }
   ],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "#choose various parameters for model\n",
    "param_grid = {'n_epochs': [5, 10,15], 'lr_all': [0.002, 0.005,0.01],\n",
    "              'reg_all': [0.2, 0.4, 0.6]}\n",
    "\n",
    "#iterate thorugh parameter grid to find best model\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "# best RMSE score\n",
    "print('best RMSE score')\n",
    "print(gs.best_score['rmse'])\n",
    "print(gs.best_score['mae'])\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print('combination of parameters that gave the best RMSE score')\n",
    "print(gs.best_params['rmse'])\n",
    "print(gs.best_params['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the best SVD model has been selected , the ***precision@k and recall@k*** could be computed with 5 folds method. As their results are very close, we could justify that data are distributed with not much outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision@k\n",
      "0.9042092753091527\n",
      "recall@k\n",
      "0.8118487226103486\n",
      "precision@k\n",
      "0.9047240901823097\n",
      "recall@k\n",
      "0.8099018479126098\n",
      "precision@k\n",
      "0.9045551391411665\n",
      "recall@k\n",
      "0.8109153981821217\n",
      "precision@k\n",
      "0.9043249646528346\n",
      "recall@k\n",
      "0.8139062717987631\n",
      "precision@k\n",
      "0.9055428117019171\n",
      "recall@k\n",
      "0.8126901223660934\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "#best algo based on above\n",
    "algo = gs.best_estimator['rmse']\n",
    "#print precision@k and recall@k\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=5, threshold=4)\n",
    "\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    print('precision@k')\n",
    "    print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "    print('recall@k')\n",
    "    print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the code below to save this model for future reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model for later use\n",
    "dump.dump(os.path.expanduser(\"SVD_GS_BEST_RMSE\"), algo=gs.best_estimator['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# matrix_factorization.SVDpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVDpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best RMSE score\n",
      "1.0289773246775415\n",
      "0.7621566029433042\n",
      "combination of parameters that gave the best RMSE score\n",
      "{'n_epochs': 15, 'lr_all': 0.01, 'reg_all': 0.2}\n",
      "{'n_epochs': 15, 'lr_all': 0.01, 'reg_all': 0.2}\n",
      "precision@k\n",
      "0.9430892627321197\n",
      "recall@k\n",
      "0.7906440639449057\n",
      "precision@k\n",
      "0.9422059919053221\n",
      "recall@k\n",
      "0.7907455676858666\n",
      "precision@k\n",
      "0.944763022451312\n",
      "recall@k\n",
      "0.7885706254019044\n",
      "precision@k\n",
      "0.9418632628431306\n",
      "recall@k\n",
      "0.7898403311048827\n",
      "precision@k\n",
      "0.9423102507053235\n",
      "recall@k\n",
      "0.7892250744462574\n"
     ]
    }
   ],
   "source": [
    "gs = GridSearchCV(SVDpp, param_grid, measures=['rmse', 'mae'], cv=3)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "# best RMSE score\n",
    "print('best RMSE score')\n",
    "print(gs.best_score['rmse'])\n",
    "print(gs.best_score['mae'])\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print('combination of parameters that gave the best RMSE score')\n",
    "print(gs.best_params['rmse'])\n",
    "print(gs.best_params['mae'])\n",
    "\n",
    "# algo = NMF()\n",
    "# cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "kf = KFold(n_splits=5)\n",
    "#best algo based on above\n",
    "algo_SVDpp = gs.best_estimator['rmse']\n",
    "#print precision@k and recall@k\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo_SVDpp.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=5, threshold=4)\n",
    "\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    print('precision@k')\n",
    "    print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "    print('recall@k')\n",
    "    print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump.dump(os.path.expanduser(\"SVDpp_GS_BEST_RMSE\"), algo=gs.best_estimator['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 Similarity Modules\n",
    "\n",
    "### 7.1 Cosine \n",
    "\n",
    "Since there are not many parameters to be tested for a similarity module, we can simple set up model and cross validate them with K folds, the code below computes the RMSE and MAE result plus their fir and test running time.\n",
    "As RMSE and MAE results are similar for 5 folds, we can specify that there are not much skewness for the dataset.\n",
    "\n",
    "The documentation for Cosine Similarity could be refered to https://surprise.readthedocs.io/en/stable/similarities.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating RMSE, MAE of algorithm KNNBaseline on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.0862  1.0859  1.0781  1.0855  1.0827  1.0837  0.0030  \n",
      "MAE (testset)     0.7404  0.7391  0.7351  0.7391  0.7369  0.7381  0.0019  \n",
      "Fit time          39.62   41.71   49.48   38.45   40.03   41.86   3.95    \n",
      "Test time         7.05    6.45    5.80    5.75    6.06    6.22    0.48    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([1.08620342, 1.08590904, 1.07812575, 1.08548604, 1.08273486]),\n",
       " 'test_mae': array([0.74041087, 0.73912286, 0.73505521, 0.73914556, 0.73690182]),\n",
       " 'fit_time': (39.62233304977417,\n",
       "  41.70804286003113,\n",
       "  49.47762894630432,\n",
       "  38.45476675033569,\n",
       "  40.02991199493408),\n",
       " 'test_time': (7.045154809951782,\n",
       "  6.44753098487854,\n",
       "  5.803295135498047,\n",
       "  5.749550104141235,\n",
       "  6.057494163513184)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_options = {'name': 'cosine', 'user_based': False} # or item based\n",
    "algo = KNNBaseline(sim_options=sim_options)\n",
    "cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view their ***precision@k and recall@k*** could be computed with 5 folds. Also their results are very close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "precision@k\n",
      "0.8961830077348748\n",
      "recall@k\n",
      "0.792255998065553\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "precision@k\n",
      "0.8966896498889148\n",
      "recall@k\n",
      "0.7901736035234929\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "precision@k\n",
      "0.8975790343687602\n",
      "recall@k\n",
      "0.788469409113767\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "precision@k\n",
      "0.8959885397779883\n",
      "recall@k\n",
      "0.7931027252017093\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "precision@k\n",
      "0.8982012734753935\n",
      "recall@k\n",
      "0.788705855925141\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "#best algo based on above\n",
    "sim_options = {'name': 'cosine', 'user_based': False} # or item based\n",
    "algo = KNNBaseline(sim_options=sim_options)\n",
    "#print precision@k and recall@k\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=5, threshold=4)\n",
    "\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    print('precision@k')\n",
    "    print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "    print('recall@k')\n",
    "    print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly , save the model on local disk for future reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model for later use\n",
    "dump.dump(os.path.expanduser(\"KNN_COSINE\"), algo=algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNNWithMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNWithMeans #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating RMSE, MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.1062  1.1044  1.1056  1.1098  1.1064  1.1065  0.0018  \n",
      "MAE (testset)     0.7556  0.7540  0.7545  0.7595  0.7584  0.7564  0.0022  \n",
      "Fit time          49.68   41.42   40.79   53.42   57.10   48.48   6.47    \n",
      "Test time         5.49    5.71    6.07    6.74    6.10    6.02    0.43    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([1.10618382, 1.10443725, 1.10556729, 1.10976373, 1.10643406]),\n",
       " 'test_mae': array([0.75555747, 0.75400287, 0.75446297, 0.75954783, 0.75838677]),\n",
       " 'fit_time': (49.68032908439636,\n",
       "  41.42016577720642,\n",
       "  40.78865575790405,\n",
       "  53.41527223587036,\n",
       "  57.098381996154785),\n",
       " 'test_time': (5.485365867614746,\n",
       "  5.706478118896484,\n",
       "  6.074939966201782,\n",
       "  6.73696494102478,\n",
       "  6.098551988601685)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_options = {'name': 'cosine', 'user_based': False} # or item based\n",
    "algo = KNNWithMeans(k=40, min_k=1,sim_options=sim_options) \n",
    "cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "precision@k\n",
      "0.8939064725067764\n",
      "recall@k\n",
      "0.7758361322372259\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "precision@k\n",
      "0.8932811743829661\n",
      "recall@k\n",
      "0.7753662129344113\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "precision@k\n",
      "0.8926267351556308\n",
      "recall@k\n",
      "0.7781896418478791\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "precision@k\n",
      "0.8918243682379483\n",
      "recall@k\n",
      "0.7752623755221357\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "precision@k\n",
      "0.8920565436524373\n",
      "recall@k\n",
      "0.7775257432824312\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "#best algo based on above\n",
    "sim_options = {'name': 'cosine', 'user_based': False} # or item based\n",
    "algo = KNNWithMeans(k=40, min_k=1,sim_options=sim_options) \n",
    "#print precision@k and recall@k\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=5, threshold=4)\n",
    "\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    print('precision@k')\n",
    "    print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "    print('recall@k')\n",
    "    print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model for later use\n",
    "dump.dump(os.path.expanduser(\"KNNWithMeans_cosine\"), algo=algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SlopeOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SlopeOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SlopeOne on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.1355  1.1314  1.1321  1.1331  1.1393  1.1343  0.0029  \n",
      "MAE (testset)     0.7590  0.7520  0.7553  0.7540  0.7600  0.7561  0.0030  \n",
      "Fit time          19.79   19.11   17.37   18.91   21.15   19.27   1.23    \n",
      "Test time         5.53    4.88    5.41    5.08    4.69    5.12    0.32    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([1.13550877, 1.13144892, 1.13210966, 1.13305882, 1.13928786]),\n",
       " 'test_mae': array([0.75903826, 0.75198709, 0.75527358, 0.75398389, 0.75999962]),\n",
       " 'fit_time': (19.79092502593994,\n",
       "  19.11101007461548,\n",
       "  17.365437030792236,\n",
       "  18.911175966262817,\n",
       "  21.147334098815918),\n",
       " 'test_time': (5.526959180831909,\n",
       "  4.880213022232056,\n",
       "  5.414744138717651,\n",
       "  5.080150127410889,\n",
       "  4.6894800662994385)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = SlopeOne()\n",
    "cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision@k\n",
      "0.907304304534314\n",
      "recall@k\n",
      "0.7576945504083302\n",
      "precision@k\n",
      "0.9068103112171201\n",
      "recall@k\n",
      "0.7554976560210354\n",
      "precision@k\n",
      "0.9089315257352952\n",
      "recall@k\n",
      "0.7578406741491446\n",
      "precision@k\n",
      "0.9094872383576333\n",
      "recall@k\n",
      "0.7536212582420128\n",
      "precision@k\n",
      "0.9072505249283516\n",
      "recall@k\n",
      "0.7548103987324362\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "#best algo based on above\n",
    "algo = SlopeOne()\n",
    "#print precision@k and recall@k\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=5, threshold=4)\n",
    "\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    print('precision@k')\n",
    "    print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "    print('recall@k')\n",
    "    print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump.dump(os.path.expanduser(\"SlopeOne_MSD\"), algo=algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import CoClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm CoClustering on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.0791  1.0776  1.0776  1.0808  1.0773  1.0785  0.0013  \n",
      "MAE (testset)     0.7282  0.7271  0.7274  0.7292  0.7257  0.7275  0.0012  \n",
      "Fit time          23.31   23.35   22.95   22.92   25.14   23.53   0.82    \n",
      "Test time         1.15    1.14    1.13    0.99    1.00    1.08    0.07    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([1.07908923, 1.07764887, 1.07762735, 1.08081944, 1.07728414]),\n",
       " 'test_mae': array([0.72819108, 0.72708234, 0.72736888, 0.72915683, 0.72565143]),\n",
       " 'fit_time': (23.31475591659546,\n",
       "  23.348692893981934,\n",
       "  22.952141046524048,\n",
       "  22.9163761138916,\n",
       "  25.140856981277466),\n",
       " 'test_time': (1.14695405960083,\n",
       "  1.1443917751312256,\n",
       "  1.1290650367736816,\n",
       "  0.9937000274658203,\n",
       "  1.0017008781433105)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = CoClustering()\n",
    "cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision@k\n",
      "0.9174858179646777\n",
      "recall@k\n",
      "0.7462795498704449\n",
      "precision@k\n",
      "0.9202601719197723\n",
      "recall@k\n",
      "0.7426032055153045\n",
      "precision@k\n",
      "0.9196174135409743\n",
      "recall@k\n",
      "0.7472589050036618\n",
      "precision@k\n",
      "0.9188577445912317\n",
      "recall@k\n",
      "0.7494450486868863\n",
      "precision@k\n",
      "0.9192650828264296\n",
      "recall@k\n",
      "0.7427458714847792\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "#best algo based on above\n",
    "algo = CoClustering()\n",
    "#print precision@k and recall@k\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=5, threshold=4)\n",
    "\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    print('precision@k')\n",
    "    print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "    print('recall@k')\n",
    "    print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump.dump(os.path.expanduser(\"CoClustering_cosine\"), algo=algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.0 Top N Recommender \n",
    "\n",
    "### 8.1 Get Top N Items \n",
    "\n",
    "This approach is based on the the top-10 items with highest rating prediction for each user in the dataset. The input could be user rating to different itmes (i.e. 20 items), then , it will return top 10 best prediction items.\n",
    "\n",
    "Below is a simple example showing the input and output for the top 10 items recommended for a user based on its prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Get the k nearest neighbors of a user (or item)\n",
    "\n",
    "We can use the get_neighbors() methods of the algorithm object. This is only relevant for algorithms that use a ***similarity measure***, such as the k-NN algorithms.\n",
    "\n",
    "Below is an example where we retrieve the 10 nearest neighbors of one of the video games from the video game review dataset. The output is represneted as 10 rows from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.get_neighbors(int(10000), k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.0 Presentation of Web App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.0 Recommendation Engine in System Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
